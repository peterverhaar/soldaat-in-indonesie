{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoeken op basis van Text & Data Mining\n",
    "\n",
    "\n",
    "In dit tutorial vind je uitleg over hoe je het corpus van \"Soldaat in Indonesie\" kunt doorzoeken op basis van Text & Data Mining. Er wordt hierbij gebruik gemaakt van de programmeertaal Python. Dit tutorial is echter geen basis-introductie tot programmeren in Python. De voorbeelden hieronder laten alleen zien hoe je dit corpus kunt doorzoeken met behulp van bestaande modules en bibliotheken. Modules zijn kant en klare en herbruikbare ‘pakketjes’ software code waarin specifieke functionaliteiten worden aangeboden. De meeste modules zijn generiek, en kunnen dus op verschillende datasets worden toegepast waarbij je dan alleen bijvoorbeeld de zoekterm moet aanpassen of het bestand waarin je wilt zoeken.\n",
    "\n",
    "\n",
    "## Installeren en importeren van modules\n",
    "\n",
    "\n",
    "In dit tutorial maken we gebruik van de module ‘os’, dat staat voor ‘operating system’. De module biedt een aantal functies waarmee je contact kunt maken met het besturingssysteem van je computer. Een van die functies is dat je de inhoud van een map op je computer kunt lezen.\n",
    "\n",
    "'nltk' is een verzameling modules die je kunt gebruiken bij analyses op het gebied van Natural Language Processing. Zo kun je paragrafen op laten splitsen in afzonderlijke zinnen, je kunt de stam van een woord of een werkwoord vinden, en je kunt de software opdracht geven om grammaticale categorieën toe te voegen aan woorden (werkwoord, eigennaam, lidwoord, zelfstandig naamwoord).\n",
    "\n",
    "De module ‘kitlvTdm’ is specifiek ontwikkeld voor dit KITLV corpus van memoires en bevat een aantal basisinstructies op op het gebied van Text & Data Mining.\n",
    "\n",
    "Als je Python hebt geïnstalleerd via Anaconda en als je hiernaast ook alle benodigde bestanden uit de Github-repository op de juiste manier hebt gekopieerd naar je computer, hoef je deze modules niet meer afzonderlijk te installeren. Anaconda bevat namelijk alle benodigde modules. \n",
    "\n",
    "Als je Python op een andere manier hebt geïnstalleerd, dan is het wellicht wel nodig om die modules te installeren. Je kunt dit vergelijken met het installeren van een nieuw programma op je computer. Na de installatie komen alle functionaliteiten van deze module of bibliotheek beschikbaar. Modules en bibliotheken kunnen via de onderstaande commando's worden geïnstalleerd. Plaats de cursor in de onderstaande cel, en klik daarna op [shift] + [Enter]. Hierna verschijnen er, als het goed is, een aantal meldingen over het installatieproces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --yes --prefix {sys.prefix} os\n",
    "!conda install --yes --prefix {sys.prefix} nltk\n",
    "!conda install --yes --prefix {sys.prefix} matplotlib \n",
    "!conda install --yes --prefix {sys.prefix} re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als alle modules correct zijn geïnstalleerd kunnen ze in de code worden geïmporteerd. Zo'n import zorgt er vervolgens voor dat alle code ook in het huidige programma gebruikt kunnen worden. \n",
    "\n",
    "Plaats de cursor in de onderstaande cel , en klik daarna op [shift] + [Enter]. Als alle modules goed zijn geïnstalleerd verschijnen er hierna geen meldingen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join , isdir\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "from kitlvTdm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Zoeken naar een specifieke term\n",
    "\n",
    "Je kunt op de eerste plaats op zoek gaan naar passages met een bepaald trefwoord in de verzameling egodocumenten. In de onderstaande code moet dit trefwoord worden opgegeven als waarde van de variabele `search_term` (bv. “Baboe” of “geweer”). Binnen de aanhalingstekens kan de zoekterm dus worden aangepast. \n",
    "\n",
    "Wanneer de code wordt uitgevoerd toont het programma een lijst van alle documenten waar de opgegeven term in voorkomt, samen met alle gevonden passages. De resultaten worden weergegeven als een zogenaamde ‘concordantie’. Zo'n concordantie wordt ook wel een *keyword in context* (KWIC) lijst genoemd. Dit betekent dat de term die je zoekt wordt weergegeven met een specifiek aantal woorden voor en na de opgegeven zoekterm. De grootte van deze passages kan worden bepaald met de variabele `window`. Het getal dat wordt opgegeven bepaalt het aantal woorden voorafgaand aan en volgend op de gebruikte term. Als die op bv. op 3 staat, en je zoekt op “geweer”, dan kun je een zin terugkrijgen als ‘ik pak mijn geweer en schiet op’. Het odnerstaande voorbeeld gebruikt de term ‘baboe’ oftewel ‘wasvrouw’. Dit is een woord dat in veel van de ego-documenten voorkomt. \n",
    "\n",
    "Tijdens het digitaliseren van de egodocumenten in het corpus van 'Soldaat in Indonesië' hebben alle documenten een eigen numerieke code gekregen. Deze codes zijn ook gebruikt in de bestandsnamen. De functie 'showTitle()', in de module kitlvTdm, zoekt de volledige titels bij deze documentcodes. Die titels worden gehaald uit het bestand 'metadata.csv', dat te vinden is in de verzameling documenten van de Master \"Soldaat in Indonesië\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'corpus'\n",
    "search_term = 'baboe'\n",
    "window = 10\n",
    "\n",
    "for file in os.listdir( dir ):\n",
    "    if re.search( '[.]txt$' , file ):\n",
    "        book = open( join( dir , file ) )\n",
    "        if re.search( search_term , book.read() , re.IGNORECASE ):\n",
    "            title = showTitle(file)\n",
    "            matches = concordance( join( dir , file ) , search_term , window )\n",
    "            if len(matches)>1:\n",
    "                print( '\\n\\nOccurrences in {} ({}):\\n'.format( title , file ) )\n",
    "                for match in matches:\n",
    "                    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dubbelklik in de bovenstaande cel en klik daarna op [shift] + [Enter]. In de vierkante haken linksboven buiten het veld, verschijnt dan een asterisk. Dit betekent dat de zoekactie in uitvoering is. Deze is afgerond als de asterisk verandert in een getal. \n",
    "\n",
    "Als je een ‘error’ krijgt, ga dan naar dit lijstje van de meest voorkomende errors met tips over hoe je die kunt oplossen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de bovenstaande code worden de resultaten simpelweg getoond in dit Notebook. Het kan uiteraard ook nuttig zijn om de resultaten te exporteren naar een tekstbestand, zodat deze resultaten dan ook weer in andere programma's (zoals Google sheets of Excel) kunnen worden bekeken of worden geanalyseerd. In die andere programma's kun je kolommen toevoegen, met, bijvoorbeeld, eigen commentaar of codes. \n",
    "\n",
    "Als je de resultaten van deze code wilt bewaren kun je deze naar een bestand exporteren via de onderstaande code. Die komen terecht op dezelfde plaats als waar dit notebook is opgeslagen. Het exportbestand heeft het *Comma Separated Value* (CSV) formaat. De naam van dit tekstbestand wordt bepaald door de variabele 'outFile'. De onderstaande code werkt alleen als eerst de bovenstaande code is uitgevoerd. \n",
    "\n",
    "In dit export bestand wordt ook het aantal treffers geteld. Naast het totaal aantal treffers wordt ook het aantal treffers per egodocument berekend.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = 'concordantieExport.csv'\n",
    "\n",
    "out = open( outFile , 'w' )\n",
    "out.write( 'total_count,count_within_document,file,title,fragment\\n' )\n",
    "\n",
    "totalCount = 0\n",
    "\n",
    "\n",
    "for file in os.listdir( dir ):\n",
    "    if re.search( '[.]txt$' , file ):\n",
    "        documentCount = 0 \n",
    "\n",
    "        \n",
    "        book = open( join( dir , file ) )\n",
    "        if re.search( search_term , book.read() , re.IGNORECASE ):\n",
    "            title = showTitle(file)\n",
    "            title = re.sub( ',' , '' , title )\n",
    "            out.write(\"Occurrences in {} ({})\\n\".format( title , file ))\n",
    "\n",
    "            matches = concordance( join( dir , file ) , search_term , window )\n",
    "            for match in matches:\n",
    "                totalCount += 1\n",
    "                documentCount += 1\n",
    "                out.write('{},{},{},{},{}\\n'.format( totalCount , documentCount , file, title, match ) )\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als je naar de resultaten van, bijvoorbeeld ‘baboe’ kijkt met 4 woorden ervoor en erna, blijkt het lastig duiding te geven aan de variëteit van de context waarin het woord voorkomt. Dit eerste hele basale voorbeeld is gekozen om het principe duidelijk te maken. We moeten daarom nu de opdracht uitbreiden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je zult je misschien afvragen waarom je met Python zou moeten zoeken, als je passages met een trefwoord ook in een gewone online zoekomgeving zou kunnen vinden. Het verschil is dat je met Python code veel flexibeler bent, en zelf kunt bepalen op welke manier gezocht wordt en in welke vorm de resultaten getoond worden. Ook is het makkelijker de resultaten te exporteren om ze verder te kunnen bewerken. \n",
    "\n",
    "**Oefening 1: Experimenteer eerst met een andere zoektermen en een ander bereik, bijvoorbeeld 20 woorden er voor en er na, door de waarden van de variabalen `search_term` en `window` aan te passen. Klik vervolgens op [Shift] + [Enter].**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collocatie - het tellen van woorden in de buurt van een term \n",
    "\n",
    "Net als bij een concordantie richt een collocatie-analyse zich op de context van specifieke zoektermen. Het verschil is dat bij een collocatie-analyse alle woorden in de context worden geteld. Op deze manier kan er een beeld ontstaan van de woorden die veel in de omgeving van een specifieke zoekterm worden gebruikt. \n",
    "\n",
    "In de code hieronder verwijst `search_term` weer naat de term waarnaar wordt gezocht, en `window` bepaalt weer het aantal woorden voor en na de opgegeven zoekterm.\n",
    "\n",
    "In de onderstaande code wordt ook de functie 'removeStopwords()' gebruikt. Deze functie heeft als effect dat de woorden die in alle documenten gemiddeld genomen even vaak voorkomen (bv. de, het, een wij, zijn, hebben, geweest, allen, doen, ik, jullie, etc.) en dus niet onderscheidend zijn voor een bepaald document, verwijderd worden. \n",
    "\n",
    "De onderstaande code zoekt in het gehele corpus, dat uit ca. 600 egodocumenten bestaat, en het uitvoeren van de code kan daarom enige tijd in beslag nemen. De analyse kan worden toegespitst op een specifieke periode. In de onderstaande code geeft de variabele 'start' het begin van de periode aan, en de variabele 'end' het einde. De code berekent alleen de woordfrequenties in de documenten die gepubliceerd zijn binnen de periode die op deze manier is vastgelegd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'corpus'\n",
    "searchTerm = 'baboe'\n",
    "window = 30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpusFreq = dict()\n",
    "\n",
    "for file in os.listdir( dir ):\n",
    "    if re.search( '[.]txt$' , file ):\n",
    "        print(\"Reading {}  ...\".format( file ))\n",
    "        freq = collocation( join( dir , file ) , searchTerm , window )\n",
    "        freq = removeStopwords( freq )\n",
    "        corpusFreq.update(freq)\n",
    "        \n",
    "        \n",
    "freq.clear()\n",
    "freq.update( removeStopwords( corpusFreq ) )        \n",
    "\n",
    "\n",
    "def sortedByValue( dict ):\n",
    "    return sorted( dict , key=lambda x: dict[x])\n",
    "\n",
    "max = 30\n",
    "i = 0\n",
    "\n",
    "if len(freq)> 0:\n",
    "\n",
    "    print( f'The following words are used most frequently in the vicinity of \"{ searchTerm }\": \\n' )\n",
    "\n",
    "    for f in reversed( sortedByValue( freq ) ):\n",
    "        i += 1\n",
    "        print( '{} =>  {}'.format( f , freq[f] ) )\n",
    "        if i == max:\n",
    "            break\n",
    "            \n",
    "else:\n",
    "    print('\\n\\nThe search term you provided does not occur in the documents published during the selected period.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wanneer je de code voor de collocatie-analyse hebt uitgevoerd kun je het resultaat opslaan met de code in de onderstaande cel. Deze code maakt een bestand aan met de naam 'collocation.csv' waarin de woorden en de frequenties terug te vinden zijn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = 'collocation.csv'\n",
    "out = open( outFile , 'w' )\n",
    "\n",
    "out.write('term,frequency\\n')\n",
    "\n",
    "for f in reversed( sortedByValue( freq ) ):\n",
    "    i += 1\n",
    "    out.write( '{},{}\\n'.format( f , freq[f] ) )\n",
    "    if i == max:\n",
    "        break\n",
    "        \n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oefening 2: Voer een collocatie-analyse uit, aan de hand van een zoekterm die van belang kan zijn voor jouw onderzoek. Experimenteer met verschillende waarden voor de variabelen `search_term`, `window`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zoeken naar een combinatie van termen \n",
    "\n",
    "Als je op zoek bent naar samenhang tussen twee zaken wil je testen of de termen die naar die zaken verwijzen inderdaad in combinatie met elkaar voorkomen. Dan moet je bepalen op welke varianten van die twee termen je wilt zoeken en wat de minimale en maximale afstand tussen die termen moet zijn. \n",
    "\n",
    "In de onderstaande code kun je op de eerste plaats twee zoektermen opgeven, als waarden van `search_term1` en `search_term2`. De variabele `window` specificeert de maximale afstand tussen deze twee woorden. \n",
    "\n",
    "De code gaat vervolgens op zoek naar alle passages van de opgegeven lengte waar de beide zoektermen in voorkomen. De gevonden passages worden in dit notebook getoond wanneer de code wordt uitgevoerd, en de resultaten worden eveneens weggeschreven in een bestand met de naam 'cooccurrence.txt'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitlvTdm import *\n",
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "\n",
    "\n",
    "search_term1 = 'soldaat'\n",
    "search_term2 = 'geweer'\n",
    "window = 30\n",
    "\n",
    "dir = \"Corpus\"\n",
    "\n",
    "\n",
    "out = open( 'cooccurrence.txt' , 'w' , encoding = 'utf-8' )\n",
    "\n",
    "search_term = searchTerm1.lower()\n",
    "search_term = searchTerm2.lower()\n",
    "pattern = '####'\n",
    "\n",
    "for file in os.listdir( dir ):\n",
    "    print( \"Reading \" + file + ' ... ' )\n",
    "    hits = []\n",
    "    if re.search( 'txt$' , file ):\n",
    "        with open( join( dir , file) ) as egodocument:\n",
    "            fullText = egodocument.read()\n",
    "        fullText = re.sub( 'page\\s+\\d+', '' , fullText )\n",
    "        fullText =  fullText.lower()\n",
    "        words = tokenise(fullText)\n",
    "\n",
    "        for i in range( 0 , (len(words)) ):\n",
    "            fragment = ' '.join( words[ i : window+i ] )\n",
    "            if re.search( r'\\b{}\\b'.format(searchTerm1) , fragment , re.IGNORECASE ) and re.search( r'\\b{}\\b'.format(searchTerm2) , fragment , re.IGNORECASE ):\n",
    "                if not( re.search( pattern , fragment , re.IGNORECASE ) ):\n",
    "                    hits.append( fragment )\n",
    "            \n",
    "                    for findex in range( 0 , window ):\n",
    "                        if re.search( r'\\b{}\\b'.format(searchTerm1) , words[i + findex] , re.IGNORECASE ) or re.search( r'\\b{}\\b'.format(searchTerm2) , words[i + findex] , re.IGNORECASE ):\n",
    "                            break\n",
    "                    pattern = ' '.join( words[ i + findex : window+i  ] )\n",
    "\n",
    "        \n",
    "            i = window+i\n",
    "    if len(hits) > 0:\n",
    "        title = showTitle(file)\n",
    "        out.write(\"Occurrences in {} ({})\\n\".format( title , file ))\n",
    "        print(\"Occurrences in {} ({})\\n\".format( title , file ))\n",
    "        for h in hits:\n",
    "            print(h + '\\n')\n",
    "            out.write(h + '\\n')\n",
    "\n",
    "out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ofening 3: Ga, met behulp van de bovenstaande code, op zoek naar passages waarin twee relevante zoektermen in voorkomen. Pas de waarden van de variabelen `search_term1` en `search_term2` aan. Indien nodig kun je ook de waarde van `window` aanpassen.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Woordfrequenties - hoe vaak komt een woord voor in een document of een corpus? \n",
    "\n",
    "Welke woorden komen het meeste voor in een bepaald egodocument? De onderstaande code berekent de frequenties van alle woorden in een enkele tekst. De tekst waarin wordt gezocht wordt bepaald door de waarde van de variabele `egodocument`. De code toont vervolgens de 30 meest voorkomende woorden in deze tekst. De variabele `max` het aantal woorden dat wordt getoond. \n",
    "\n",
    "Ook hier is het van belang de stopwoordenlijst te gebruiken om alleen zinvolle frequenties in de resultaten terug te krijgen die te maken hebben met de inhoud, en om niet ter zake doende lidwoorden, bijwoorden en voorzetsels er uit te filteren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'corpus'\n",
    "egodocument = '03391.txt'\n",
    "max = 30\n",
    "\n",
    "\n",
    "freq = calculateWordFrequencies( join( dir , egodocument ) )\n",
    "freq = removeStopwords( freq )\n",
    "\n",
    "sorted_f = sorted( freq , key=lambda x: freq[x])\n",
    "\n",
    "i = 0\n",
    "\n",
    "print( f'The following words occur most frequently in the text { egodocument } ({ showTitle( egodocument )}).\\n' )\n",
    "\n",
    "for f in reversed( sorted_f ):\n",
    "    i += 1\n",
    "    print( '{} => {}'.format( f , freq[f] ) )\n",
    "    if i == max:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oefening 4: Bepaal de meest frequente woorden in een van de egodocumenten in het corpus van \"Soldaat in Indonesië\". Experimenteer met verschillende waarden voor de variabelen `egodocument` en `max`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De woordfrequenties geven deels een vertekend beeld, omdat in veel memoires letterlijk passages of hele delen van dagboeken uit de tijd zelf worden gebruikt. Eigenlijk zou je die boeken die verhoudingsgewijze veel van deze teksten bevatten moeten kunnen isoleren, zodat de boeken zonder deze passages kunnen worden geanalyseerd. Dat kan alleen door extra codes aan de publicaties toe te voegen die dit verschil aangeeft. Die laag is er nu nog niet, maar staat wel in de planning. \n",
    "\n",
    "Veranderingen in woordgebruik zijn gebonden aan tijd en veranderingen in de samenleving. Omdat dit corpus zich uitstrekt vanaf de periode van het conflict zelf (1945-1949) tot aan 2017, is het interessant om het voorkomen van bepaalde termen chronologisch te vergelijken. De onderstaande code verdeelt het corpus in perioden van 5 jaar, en berekent vervolgens de woordfrequenties voor de egodocumenten die in deze verschillende tijdvakken verschenen. Hierbij moet wel de kanttekening worden geplaatst dat niet alle egodocumenten konden worden gedateerd. Bij deze analyse worden de teksten die nog niet zijn gedateerd genegeerd. Verder is het uiteraard ook zo dat er een onevenredige verdeling is van het aantal boeken over deze perioden. De absolute frequenties kunnen daardoor niet zonder meer worden vergeleken. OM de frequenties toch vergelijkbaar te maken zijn de absolute tellingen steeds gedeeld door het totaal aantal woorden in de egoducmenten uit de verschillende perioden.\n",
    "\n",
    "De lengte van de geanlyseerde periode kan overgens worden aangepast via de variabele `period_length`. \n",
    "\n",
    "De resultaten worden getoond in dit Notebook, maar worden eveneens weggeschreven in een bestand met de naam `frequency_chronological.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitlvTdm import *\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "\n",
    "start = 1930\n",
    "end = 2020\n",
    "period_length = 5\n",
    "nr_words = 20\n",
    "\n",
    "dir = 'Corpus'\n",
    "\n",
    "def sortedByValue( dict ):\n",
    "    return sorted( dict , key=lambda x: dict[x])\n",
    "\n",
    "intervals = []\n",
    "\n",
    "out = open( 'frequency_chronological.csv' , 'w' , encoding = 'utf-8' )\n",
    "out.write( 'year_from,year_to,tokenCount,term,frequency\\n' )\n",
    "\n",
    "for year in range( start , end , period_length ):\n",
    "    intervals.append(year)\n",
    "\n",
    "for year in intervals:\n",
    "    year_from = year\n",
    "    year_to =  period_length -1 + year\n",
    "\n",
    "    freqTotal = dict()\n",
    "    tokensTotal = 0\n",
    "    egodocuments = 0\n",
    "\n",
    "    for file in os.listdir(dir):\n",
    "        if re.search( 'txt$' , file ):\n",
    "            year = showYear( file )\n",
    "            if year is not None:\n",
    "                if year >= year_from and year <= year_to:\n",
    "                    egodocuments += 1\n",
    "                    freq = calculateWordFrequencies( join( dir, file ) )\n",
    "                    freqTotal.update(freq)\n",
    "                    tokensTotal += numberOfTokens( join( dir, file ) )\n",
    "\n",
    "    print( f'\\n{ year_from }-{ year_to }\\n{tokensTotal} words in total in {egodocuments} egodocuments\\n\\n' )\n",
    "\n",
    "\n",
    "    freqTotal = removeStopwords( freqTotal )\n",
    "\n",
    "    count = 0\n",
    "    for word in reversed( sortedByValue(freqTotal) ):\n",
    "        out.write( f'{year_from},{year_to},{tokensTotal},{word},{freqTotal[word]}\\n' )\n",
    "        print( f' { word } => { freqTotal[word] / tokensTotal }' )\n",
    "        count += 1\n",
    "        if count == nr_words:\n",
    "            break\n",
    "\n",
    "out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oefening 4: Probeer met behulp van de bovenstaande code te verkennen hoe het woordgebruik zich ontwikkelde over de loop van de afgelopen decennia. Verander hiervoor de waarde van de variabelen `period_length` en `nr_words`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woordsoorten als zelfstandige naamwoorden, bijvoeglijke naamwoorden en werkwoorden drukken zijn over het algemeen het meest bepalend voor de betekenis van zinnen. Het kan daarom nuttig en informatief zijn om frequentie-analyses te beperken tot dit soort woorden. In de onderstaande cellen worden uitsluitend de zelfstandige naamwoorden, bijvoeglijke naamwoorden en werkwoorden geteld, door gebruik te maken van de hierboven al genoemde module `nltk`. Deze module richt zich normaal gesproken op Engelstalige teksten. Om `nltk` ook toe te kunnen passen op Nederlandstalige teksten moet eerst de onderstaande code worden uitgevoerd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('alpino')\n",
    "\n",
    "from nltk.corpus import alpino as alp\n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "training_corpus = alp.tagged_sents()\n",
    "unitagger = UnigramTagger(training_corpus)\n",
    "bitagger = BigramTagger(training_corpus, backoff=unitagger)\n",
    "pos_tag = bitagger.tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als de installatie van de Nederlandstalige variant van `nltk` geen problemen opleverde, kan de onderstaande, meer gerichte frequentie-analyse worden uitgevoerd. Let er hierbij op dat het toekennen van grammaticale categorieën wel enige rekenkracht vergt. Het uitvoeren van de code kan dus enige tijd in beslag nemen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitlvTdm import *\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "nr_words = 100\n",
    "\n",
    "dir = 'Corpus'\n",
    "egodocument = '03391.txt'\n",
    "\n",
    "def sortedByValue( dict ):\n",
    "    return sorted( dict , key=lambda x: dict[x])\n",
    "\n",
    "\n",
    "\n",
    "out = open( 'frequency_POS.csv' , 'w' , encoding = 'utf-8' )\n",
    "out.write( 'word,frequency\\n' )\n",
    "\n",
    "\n",
    "\n",
    "freqTotal = dict()\n",
    "\n",
    "countFile = 0 \n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if re.search( 'txt$' , file ):\n",
    "        countFile += 1\n",
    "        if file == egodocument:\n",
    "        #print( '\\rReading {} ... ({}/577)'.format(file , countFile ) )\n",
    "            with open( join( dir, file ) ) as fileName:\n",
    "                fullText = fileName.read()\n",
    "                sent = sent_tokenize(fullText)\n",
    "                for s in sent:\n",
    "                    words = word_tokenize(s)\n",
    "                    pos = pos_tag(words)\n",
    "                    for p in pos:\n",
    "                        if p[1] is not None:\n",
    "                            if re.search( r'^(adj)|(noun)|(verb)' , p[1] ):\n",
    "                                freqTotal[ p[0]  ] = freqTotal.get( p[0] ,0 ) + 1\n",
    "\n",
    "\n",
    "freqTotal = removeStopwords( freqTotal )\n",
    "\n",
    "for word in reversed( sortedByValue(freqTotal) ):\n",
    "    out.write( f'{word},{freqTotal[word]}\\n' )\n",
    "    print( f' { word } => { freqTotal[word] / tokensTotal }' )\n",
    "    count += 1\n",
    "    if count == nr_words:\n",
    "        break\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oefening 5: Probeer een lijst te generen van de 150 meest frequente zelfstandige naamwoorden, bijvoeglijke naamwoorden en werkwoorden.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Distributiegrafiek\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historische ontwikkelingen kunnen ook op een andere manier worden geanalyseerd. Appplicaties zoals de [Google Books Ngram viewer](https://books.google.com/ngrams) maken gebruik van distributie-grafieken waarin woordfrequenties per jaar worden berekend. De onderstaande ocde doet iets vergelikbaar voor het corpus van \"Soldaat in Indonesië\". Zoals te zien is in de eerste regels van de onderstaande cel wordt er eerst een lljst van woorden opgegeven. Deze woorden worden allemaal vastgelegd onder de naam `search_terms`. De code berekent vervolgens per jaar hoe vaak deze woorden zijn gebruikt. De resultaten worden vastgelegd in een CSV-bestand waarin de relatieve frequenties (het aantal voorkomens va deze woorden, gedeeld door het totaal aantal woorden in dat jaar) worden vastgelegd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "search_terms = '''\n",
    "gestoken\n",
    "vuur\n",
    "schieten\n",
    "gevangen\n",
    "doodgeschoten\n",
    "schoten\n",
    "neergeschoten\n",
    "gevangenen\n",
    "patrouille\n",
    "gevangene\n",
    "dood\n",
    "geschoten\n",
    "soldaten\n",
    "doden\n",
    "bloed\n",
    "geweer\n",
    "gedood\n",
    "wapens\n",
    "beschoten\n",
    "schoot\n",
    "kogel\n",
    "gewonden\n",
    "geslagen\n",
    "vermoord\n",
    "verhoord\n",
    "zuiveringsactie\n",
    "hinderlaag\n",
    "slachtoffers\n",
    "executie\n",
    "patrouilles\n",
    "wraak\n",
    "inlichtingendienst\n",
    "extremisten\n",
    "kogels\n",
    "woede\n",
    "moord\n",
    "mortieren\n",
    "krijgsraad\n",
    "vuurgevecht\n",
    "incident\n",
    "kampement\n",
    "gewonde\n",
    "handgranaat\n",
    "vluchtende\n",
    "gevecht\n",
    "geëxecuteerd\n",
    "pistool\n",
    "gewond\n",
    "oorlog\n",
    "fik\n",
    "tegenstanders\n",
    "krijgsgevangenen\n",
    "strijd\n",
    "mitrailleur\n",
    "spion\n",
    "platgebrand\n",
    "kapot\n",
    "afgebrand\n",
    "zuivering\n",
    "bajonet\n",
    "salvo\n",
    "mishandeld\n",
    "omsingeld\n",
    "granaten\n",
    "gevangengenomen\n",
    "bataljon\n",
    "vernietigd\n",
    "represaille\n",
    "beschieting\n",
    "tegenpartij\n",
    "terroristen\n",
    "plunderaars\n",
    "doodschieten\n",
    "brandende\n",
    "hardhandig\n",
    "moordenaars\n",
    "genadeschot\n",
    "pijn\n",
    "revolver\n",
    "gemarteld\n",
    "gefusilleerd\n",
    "mes\n",
    "wapen\n",
    "aangevallen\n",
    "neerschieten\n",
    "gezuiverd\n",
    "brandde\n",
    "plunderen\n",
    "vijanden\n",
    "beschietingen\n",
    "gedreigd\n",
    "geplunderd\n",
    "slaag\n",
    "executeren\n",
    "aanslag\n",
    "geschreeuw\n",
    "handgranaten\n",
    "afschuwelijke\n",
    "gebombardeerd\n",
    "verminkt\n",
    "gemene\n",
    "burgerslachtoffers\n",
    "plundering\n",
    "plunderingen\n",
    "krijgsgevangene\n",
    "moorden\n",
    "excessief\n",
    "geweldsexcessen\n",
    "mishandeling\n",
    "gevechten\n",
    "sadistische\n",
    "vuurstoot\n",
    "weerloze\n",
    "martelen\n",
    "executies\n",
    "doodgeslagen\n",
    "schreeuwde\n",
    "schietend\n",
    "onthoofd\n",
    "represailles\n",
    "fosforgranaten\n",
    "verminkte\n",
    "gruwelijk\n",
    "nekschot\n",
    "tegenaanval\n",
    "overval\n",
    "verwond\n",
    "moordenaar\n",
    "bewapend\n",
    "opblazen\n",
    "executiepeloton\n",
    "verkracht\n",
    "vuurstoten\n",
    "vuurwapens\n",
    "vergelding\n",
    "beestachtige\n",
    "doodstraf\n",
    "trekker\n",
    "stervende\n",
    "neergeknald\n",
    "platbranden\n",
    "artillerie\n",
    "bedreiging\n",
    "slachting\n",
    "verwondingen\n",
    "vreselijke\n",
    "ontgelden\n",
    "gummiknuppel\n",
    "knallen\n",
    "gevaarlijk\n",
    "slachtoffer\n",
    "marteling\n",
    "beestachtig\n",
    "platgeschoten\n",
    "lijden\n",
    "uitgemoord\n",
    "schietpartij\n",
    "bloedbad\n",
    "mortiergranaten\n",
    "misdadigers\n",
    "massagraf\n",
    "gruwelijke\n",
    "afgehakt\n",
    "kapmes\n",
    "bombardement\n",
    "schokkende\n",
    "zuiveringsopdracht\n",
    "'''\n",
    "\n",
    "search_terms = search_terms.strip()\n",
    "terms_list = re.split( r'\\n' , search_terms.lower() )\n",
    "\n",
    "\n",
    "\n",
    "frequency_year = dict()\n",
    "word_count_year  = dict()\n",
    "    \n",
    "dir = 'corpus'\n",
    "\n",
    "corpusFreq = dict()\n",
    "\n",
    "\n",
    "\n",
    "for file in os.listdir( dir ):\n",
    "    if re.search( '[.]txt$' , file ):\n",
    "        print( f\"Reading { file } ...\" )\n",
    "        if showYear( file ) is not None:\n",
    "            year = int(showYear( file ) )\n",
    "            freq = calculateWordFrequencies( join( dir , file ) )\n",
    "            freq = removeStopwords( freq )\n",
    "            tokens = numberOfTokens( join( dir , file ) )\n",
    "            word_count_year[ year ] = word_count_year.get( year , 0 ) + tokens\n",
    "            for term in terms_list:\n",
    "                if term in freq:\n",
    "                    frequency_year[year] = frequency_year.get( year , 0 ) + freq[ term ]\n",
    "\n",
    "                \n",
    "            \n",
    "out = open( 'dispersion_data.csv', 'w' , encoding = 'utf-8' )\n",
    "out.write( 'year,frequency\\n' )\n",
    "\n",
    "for year in sorted( frequency_year  ):\n",
    "    out.write( '{},{}\\n'.format( year , frequency_year[ year ] / word_count_year[year] ) )\n",
    "    print( '{} times in {}.'.format( frequency_year[ year ] , year ) )\n",
    "\n",
    "out.close()\n",
    "\n",
    "print('The CSV file has been created.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De frequenties kunnen hierna worden gevisualiseerd met de onderstaande code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv( 'dispersion_data.csv' )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "fig = plt.figure( figsize = ( 15, 4))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot( df['year'] , df['frequency'] , color = '#930d08' , linestyle = 'solid')\n",
    "\n",
    "ax.set_xlabel('Years')\n",
    "ax.set_ylabel('Word frequency')\n",
    "\n",
    "\n",
    "ax.set_xlim( df['year'].min() , df['year'].max() )\n",
    "\n",
    "ax.set_title( 'Dispersion graph')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oefening 6: Pas de waarde van de variabele `search_terms` aan. Geef een aantal woorden op die van belang zijn voor jouw onderzoek. Probeer hierna een distributiegrafiek te maken die de historische ontwikkelingen in het gebruik van deze termen kan verduidelijken.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
